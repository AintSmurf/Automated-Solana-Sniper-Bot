# Logs & Analysis

Logs are organized for clarity and traceability:

| File / Folder                          | Description                                                  |
| -------------------------------------- | ------------------------------------------------------------ |
| `logs/info.log`                        | General info / high-level operational logs                  |
| `logs/debug.log`                       | Main developer-focused debug log (active file)              |
| `logs/debug/debug.log.*`              | Rotated debug chunks (large, periodically compressed or deleted) |
| `logs/backups/debug/*debug.log*`      | Archived debug chunks copied off the main machine           |
| `logs/console_logs/console.info`      | Simplified console-style view                               |
| `logs/special_debug.log`              | Critical debug logs (e.g. scam analysis, safety checks)     |
| `logs/matched_logs/<mint>.log`        | Per-token, time-sorted summaries generated by the analyzer  |

The active `logs/debug.log` is never touched by the shrinker. Only rotated chunks
(`debug.log.1`, `debug.log.2`, backups, and their `.gz` versions) are compressed or deleted.

---

## Automatic Log Maintenance & Daily Analysis

To keep disk usage under control and generate daily per-token bundles, the project includes
a small maintenance entrypoint driven by `config/logs_config.json`:

```bash
python -m bot_scripts.maintain_logs
```

### Configuration
- **`config/logs_config.json`**
  - New JSON config for log retention and optional daily analysis:
    ```json
    {
      "AUTO_ANALYZE_TOKENS": true,
      "AUTO_GZIP": true,
      "AUTO_DELETE": false,
      "RETENTION_DAYS": 2,
      "INCLUDE_BACKUPS": true
    }
    ```
### AUTO_ANALYZE_TOKENS

- If true, maintain_logs will automatically run:

```bash
python -m bot_scripts.run_analyze --today
```


- Loads today’s trades from the DB via TokenDAO.fetch_mint_signature(...),

- Spawns analyze.py in parallel for each (signature, token) pair,

- Writes compact, time-sorted logs to logs/matched_logs/<token_mint>.log.

### AUTO_GZIP / AUTO_DELETE (mutually exclusive)
- AUTO_GZIP = true → compress old rotated logs (.log → .log.gz).

- AUTO_DELETE = true → delete old rotated logs instead of compressing.

- If both are true, maintain_logs aborts with a clear error: ❌ Both AUTO_GZIP and AUTO_DELETE are True. Please choose one.

### RETENTION_DAYS
- Only files with mtime < today - RETENTION_DAYS are candidates for gzip/delete.

- Example: RETENTION_DAYS = 2 → only touch logs strictly older than 2 days.

### INCLUDE_BACKUPS
- If true, also processes backup chunks under logs/backups/debug/.

- If false, only logs/debug/debug.log.* is touched.

### Manual shrinking – shrink_logs.py

Example commands:

    # Dry-run: show what would be compressed (no changes)
    python -m bot_scripts.shrink_logs \
      --before 2025-12-05 \
      --mode gzip \
      --include-backups \
      --dry-run

    # Actually compress old logs
    python -m bot_scripts.shrink_logs \
      --before 2025-12-05 \
      --mode gzip \
      --include-backups

### Recommended usage (cron / systemd)
```bash
python -m bot_scripts.maintain_logs
```
- once per night so that:
  - Rotated logs get compressed or deleted according to logs_config.json, and
  - If AUTO_ANALYZE_TOKENS = true, fresh per-token log bundles for today’s trades appear under logs/matched_logs/.

---

## Log Summarization Tool

This tool allows you to extract, clean, and analyze logs for one or multiple token addresses and transaction signatures.

### Functionality

- Searches across:
  - `logs/debug/`
  - `logs/backup/debug/`
  - `logs/info.log`
- Matches logs by:
  - `--signature` (transaction signature)
  - `--token` (mint address)
- Removes duplicate or overlapping lines
- Sorts all matched logs chronologically
- Outputs a clean, consolidated log to:  
  `logs/matched_logs/<token_address>.log`

---

### Manual Usage (One Token)

To analyze a **single** token and transaction:
```bash
    python -m bot_scripts.analyze --signature <txn_signature> --token <token_address>
```
This will:

- Search all configured log locations for that signature and token.
- Deduplicate and time-sort all matches.
- Write the results to: `logs/matched_logs/<token_address>.log`.

---

### Batch Usage (Multiple Tokens via DB)

To analyze multiple tokens in parallel:

```bash
    python -m bot_scripts.run_analyze [options...]
```

Explanation:
- Builds a DB context  
- Queries `tokens` + `trades` via `TokenDAO.fetch_mint_signature(...)`  
- Selects only the relevant tokens (based on filters)  
- Runs `analyze.py` in parallel for each `(signature, token_address)` pair  
- Uses `max_workers=10` by default  

Available Options:

- `--reason {lost,tp,sl,tsl,timeout,manual}`  
  Filter by trades with a given `trigger_reason` / `exit_reason`.  
  Examples: LOST rugs, TP (take-profit), SL (stop-loss), etc.

- `--today`  
  Only include trades from *today*  
  (00:00 → 00:00 in the local machine's timezone).

- `--since YYYY-MM-DD`  
  Only include trades with `timestamp >=` this date (open-ended).  
  Example: `--since 2025-11-01`

- `--all`  
  Ignore trade filters and process **all tokens** from the `tokens` table.

- `--limit N`  
  Process at most **N tokens** (useful for testing / sampling).

Example commands:

    # LOST tokens today
    python -m bot_scripts.run_analyze --reason lost --today

    # TP tokens since 2025-11-01
    python -m bot_scripts.run_analyze --reason tp --since 2025-11-01

    # Any tokens with trades today (any reason)
    python -m bot_scripts.run_analyze --today

    # All tokens, but only first 20
    python -m bot_scripts.run_analyze --all --limit 20

---

## Summary Report Tool

This tool produces an Excel summary of your trades and daily performance.

To generate the summary:

```bash
    python -m bot_scripts.produce_summary
```

Explanation:

- Connects to the DB via `TokenDAO`  
- Pulls:
  - Detailed per-trade data  
  - Aggregated per-session data (07:00 → next-day 07:00, local time)  
- Writes an Excel file:  
  `summary_results.xlsx`

The Excel file currently contains:

### `Summary` – per-trade details

- Columns:  
  `token`, `buy_event`, `sell_event`,  
  `buy_price`, `sell_price`,  
  `profit_percent`, `exit_reason`,  
  `post_buy_score`, `marketcap`
- Numeric columns are formatted (prices, PnL %, marketcap).

### `PerSession` – per “trading day”

- Trading day is defined as: **07:00 → next-day 07:00** (local timezone)
- Columns:
  - `session_date`
  - `trade_count`
  - `total_pnl_percent`
  - `avg_pnl_percent`
  - `winning_trades`
  - `losing_trades`
- Uses the PC’s local timezone offset to define sessions.
- Lets you see:
  - Winning/losing streaks
  - Daily/session performance
  - Rug-heavy days vs. good days



